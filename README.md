# ğŸš€ RAG API com IA Local

Este projeto faz parte da minha jornada de aprendizado em Engenharia de Dados para IA. Aqui, construÃ­ uma API de **GeraÃ§Ã£o Aumentada de RecuperaÃ§Ã£o (RAG)** que utiliza o Ollama para rodar modelos de linguagem localmente.

## ğŸ§  Sobre o Projeto
A API foi desenvolvida com **FastAPI** e permite que novos conhecimentos sejam injetados dinamicamente. Durante os testes, "ensinei" Ã  IA fatos sobre minha vida, como minha paixÃ£o por skate e meus planos para 2027, e ela foi capaz de responder com precisÃ£o baseada nesse contexto.

## ğŸ› ï¸ Tecnologias Utilizadas
- **Python**: Linguagem base.
- **FastAPI**: CriaÃ§Ã£o dos endpoints da API.
- **Ollama (Llama 3)**: Motor de IA local.
- **Git/GitHub**: Versionamento e portfÃ³lio.

## ğŸ“ Contexto
Projeto desenvolvido em Curitiba, a "Europa brasileira".

## ğŸš€ Como rodar
1. Instale o [Ollama](https://ollama.com/) e baixe o modelo: `ollama run llama3`.
2. Instale as dependÃªncias: `pip install -r requirements.txt`.
3. Rode a API: `uvicorn app:app --reload`.
4. Acesse o Swagger em: `http://127.0.0.1:8000/docs`.

## ğŸ“ˆ PrÃ³ximos Passos
- [x] Fase 1: Build da RAG API.
- [ ] Fase 2: ContainerizaÃ§Ã£o com Docker.
- [ ] Fase 3: Deploy com Kubernetes.
- [ ] Fase 4:Automate with GitHub Actions
- [ ] Fase 5:Monitor and detect issues